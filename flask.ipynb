{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flask.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1EaEtHLfO2x8N0dQXBxm68ABWMuIwLGWW",
      "authorship_tag": "ABX9TyMvU0SXIlOhUSaVD3poTge4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KsPsD/Chatbot_server/blob/master/flask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZItnB62QnYbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md8Xu9hinaND",
        "colab_type": "code",
        "outputId": "e6521f68-4d4b-4e46-aca0-7dfd155b6177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 27 12:26:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBy3HZ8YniSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask, render_template, session, redirect, url_for, session ,request\n",
        "from flask_wtf import FlaskForm\n",
        "from wtforms import TextField,SubmitField\n",
        "from wtforms.validators import NumberRange\n",
        "import numpy as np \n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertForSequenceClassification,BertTokenizer\n",
        "import torch\n",
        "from flask_restful import Resource, Api\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loPR8cBEnrft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_prediction(model,tokenizer,sample_json):\n",
        " \n",
        "  sen_1 = sample_json['sen_1']\n",
        "  personalities=[\"may i help you yes please.\", \"do you have long - sleeve shirts?\", \"yes\", \"they are right here. how much are they? \",\"they are 15 dollars each do you have any larger sizes what about these?\", \"there are larger and more colorful.\", \"this one looks good. can i try this on? sure.\" ,\"the fitting room is over there.\", \"what do you think? that looks really good on you\", \"thanks.\", \"i will take this\"] \n",
        "  out_put= []\n",
        "  classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "  for i in personalities:\n",
        "    sequence_persona=i\n",
        "    paraphrase = tokenizer.encode_plus(sen_1, sequence_persona, return_tensors=\"pt\")\n",
        "    paraphrase_classification_logits = model(**paraphrase)[0]\n",
        "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "    out_put.append(paraphrase_results[1])\n",
        "\n",
        "\n",
        "  out_put=np.array(out_put)\n",
        "  print(out_put)\n",
        "  out_put_max=np.max(out_put)\n",
        "  results = {\"max_value\":round(out_put_max*100)}\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ExMNQan0en",
        "colab_type": "code",
        "outputId": "d154f9f7-1aa9-41d5-8944-be4220d41b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  \n",
        "M_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "M_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "@app.route('/prediction', methods = ['POST'])\n",
        "def prediction():\n",
        "    sentence = request.get_json()#json 데이터를 받아옴\n",
        "    result_mrpc =return_prediction(model=M_model,tokenizer=M_tokenizer,sample_json=sentence)\n",
        "    result_cola =return_prediction((model=C_model,tokenizer=C_tokenizer,sample_json=sentence))\n",
        "    return jsonify(results)# 받아온 데이터를 다시 전송\n",
        " \n",
        "@app.route('/environments/<language>')\n",
        "def environments(language):\n",
        "    return jsonify({\"language\":language})\n",
        " \n",
        " \n",
        "\n",
        "app.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://193f5aa3.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/May/2020 13:14:03] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.1385449  0.12725087 0.37171873 0.10501838 0.04932571 0.05844016\n",
            " 0.05679782 0.04604949 0.07768957 0.24015231 0.12996598]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/May/2020 13:14:41] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.1385449  0.12725087 0.37171873 0.10501838 0.04932571 0.05844016\n",
            " 0.05679782 0.04604949 0.07768957 0.24015231 0.12996598]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/May/2020 13:15:06] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.0616855  0.08772871 0.25182939 0.04443881 0.04178891 0.04893312\n",
            " 0.0635411  0.04885901 0.04541544 0.08292638 0.10194582]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/May/2020 13:15:35] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.1385449  0.12725087 0.37171873 0.10501838 0.04932571 0.05844016\n",
            " 0.05679782 0.04604949 0.07768957 0.24015231 0.12996598]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/May/2020 13:24:26] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.1385449  0.12725087 0.37171873 0.10501838 0.04932571 0.05844016\n",
            " 0.05679782 0.04604949 0.07768957 0.24015231 0.12996598]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PiwtF97vR7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_prediction(model,tokenizer,sample_json):\n",
        "  \n",
        " \n",
        "  sen_1 = sample_json['sen_1']\n",
        "  personalities=[\"may i help you yes please.\", \"do you have long - sleeve shirts?\", \"yes\", \"they are right here. how much are they? \",\"they are 15 dollars each do you have any larger sizes what about these?\", \"there are larger and more colorful.\", \"this one looks good. can i try this on? sure.\" ,\"the fitting room is over there.\", \"what do you think? that looks really good on you\", \"thanks.\", \"i will take this\"] \n",
        "  out_put= []\n",
        "  classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "  for i in personalities:\n",
        "    sequence_persona=i\n",
        "    paraphrase = tokenizer.encode_plus(sen_1, sequence_persona, return_tensors=\"pt\")\n",
        "    paraphrase_classification_logits = model(**paraphrase)[0]\n",
        "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "    out_put.append(paraphrase_results[1])\n",
        "\n",
        "\n",
        "  out_put=np.array(out_put)\n",
        "  print(out_put)\n",
        "  out_put_max=np.max(out_put)\n",
        "  results = {\"max_value\":round(out_put_max*100)}\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-M-s95OoipE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification,BertTokenizer\n",
        "C_model =  BertForSequenceClassification.from_pretrained(\"/content/drive/My Drive/CoLAoutput\")\n",
        "C_tokenizer= BertTokenizer.from_pretrained(\"/content/drive/My Drive/CoLAoutput\")  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBS3YLLNuO5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "classes = [\"wrong\", \"correct\"]\n",
        "\n",
        "sequence_0 = \"\"\n",
        "paraphrase = tokenizer2.encode_plus(sequence_0, return_tensors=\"pt\")\n",
        "# not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "\n",
        "paraphrase_classification_logits = model2(**paraphrase)[0]\n",
        "# not_paraphrase_classification_logits = model(**not_paraphrase)[0]\n",
        "\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "# not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "\n",
        "\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77VzvXwW2K6E",
        "colab_type": "text"
      },
      "source": [
        "# 여기는 oop 로"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LwkXsEQ-BEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask, render_template, session, redirect, url_for, session ,request\n",
        "from flask_wtf import FlaskForm\n",
        "from wtforms import TextField,SubmitField\n",
        "from wtforms.validators import NumberRange\n",
        "import numpy as np \n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertForSequenceClassification,BertTokenizer\n",
        "import torch\n",
        "from flask_restful import Resource, Api\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tta-51awEw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AFL:\n",
        "  count=0\n",
        "  change_flag=False\n",
        "  def __init__(self,model,tokenizer):\n",
        "    self.model = model\n",
        "    self.tokenizer= tokenizer\n",
        "    self.score=[]\n",
        "    \n",
        "\n",
        "  def return_prediction(self,sample_json):\n",
        "    sen_1 = sample_json['sen_1']\n",
        "    out_put= []\n",
        "    if self.model == M_model:\n",
        "      print(self.model)\n",
        "      personalities=[\"may i help you yes please.\", \"do you have long - sleeve shirts?\", \"yes\", \"they are right here. how much are they? \",\"they are 15 dollars each do you have any larger sizes what about these?\", \"there are larger and more colorful.\", \"this one looks good. can i try this on? sure.\" ,\"the fitting room is over there.\", \"what do you think? that looks really good on you\", \"thanks.\", \"i will take this\"] \n",
        "      classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "      for i in personalities:\n",
        "        sequence_persona=i\n",
        "        paraphrase = self.tokenizer.encode_plus(sen_1, sequence_persona, return_tensors=\"pt\")\n",
        "        paraphrase_classification_logits = self.model(**paraphrase)[0]\n",
        "        paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "        out_put.append(paraphrase_results[1])\n",
        "      out_put=np.array(out_put)\n",
        "      print(out_put)\n",
        "      out_put_max=np.max(out_put)\n",
        "      self.score.append(round(out_put_max*100))\n",
        "      # if AFL.count==10:\n",
        "      #   avg =average(self.score,AFL.count)\n",
        "      #   return avg\n",
        "      \n",
        "      results = {\"max_value\":round(out_put_max*100)}\n",
        "\n",
        "      return round(out_put_max*100)\n",
        "\n",
        "    elif self.model == C_model:\n",
        "      classes = [\"wrong\", \"correct\"]\n",
        "      paraphrase = self.tokenizer.encode_plus(sen_1, return_tensors=\"pt\")\n",
        "      paraphrase_classification_logits = self.model(**paraphrase)[0]\n",
        "      paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "      self.score.append(round(paraphrase_results[1] * 100))\n",
        "      # if AFL.count==10:\n",
        "      #   avg =average(self.score,AFL.count)\n",
        "      #   return avg\n",
        "      # results = {classes[1]: paraphrase_results[1]}\n",
        "      return round(paraphrase_results[1] * 100)\n",
        "  \n",
        "  def average(self):\n",
        "    return sum(self.score)/AFL.count\n",
        "  \n",
        "  @staticmethod\n",
        "  def change_content(CoLA,MRPC):\n",
        "    CoLA_avg = CoLA.average()\n",
        "    MRPC_avg = MRPC.average()\n",
        "    if CoLA_avg >80 and MRPC_avg >70:\n",
        "      \n",
        "    ##change contents  컨텐츠 바꿔주는 코드 오면 될듯 그 공용 contents 변수가 있을거임 그 때는\n",
        "    AFL.change_flag=False\n",
        "    return #바뀐 컨텐츠 주기\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGm6WNfN2jDT",
        "colab_type": "code",
        "outputId": "8f904957-d23a-47ca-ad85-85d57082c1a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from . import AFL\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  \n",
        "\n",
        "M_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "M_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "C_model =  BertForSequenceClassification.from_pretrained(\"/content/drive/My Drive/CoLAoutput\")\n",
        "C_tokenizer= BertTokenizer.from_pretrained(\"/content/drive/My Drive/CoLAoutput\")  \n",
        "\n",
        "CoLA =AFL(C_model,C_tokenizer)\n",
        "MRPC =AFL(M_model,M_tokenizer)\n",
        "\n",
        "@app.route('/prediction', methods = ['POST'])\n",
        "def prediction():\n",
        "    if AFL.count ==10:\n",
        "      # AFL.change_flag=True\n",
        "      # AFL.change_content(CoLA,MRPC)\n",
        "      # AFL.count =0\n",
        "      return redirect(url_for('change_content'))\n",
        "    sentence = request.get_json()#json 데이터를 받아옴\n",
        "    result_mrpc =MRPC.return_prediction(sample_json=sentence)\n",
        "    result_cola =CoLA.return_prediction(sample_json=sentence)\n",
        "    results={\"similarity\":result_mrpc,\"correct\":result_cola}\n",
        "    AFL.count+=1\n",
        "    return jsonify(results)# 받아온 데이터를 다시 전송\n",
        " \n",
        "@app.route('/environments/<language>')\n",
        "def environments(language):\n",
        "    return jsonify({\"language\":language})\n",
        "\n",
        "@app.route('/prediction/change')\n",
        "def change_content():\n",
        "\n",
        "\n",
        "app.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://61282a37.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/May/2020 14:45:47] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.1385449  0.12725087 0.37171873 0.10501838 0.04932571 0.05844016\n",
            " 0.05679782 0.04604949 0.07768957 0.24015231 0.12996598]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqjs5qeLKEI6",
        "colab_type": "text"
      },
      "source": [
        "# 여기서 부터 convAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAikSMQcKKF5",
        "colab_type": "code",
        "outputId": "d6088bea-25fb-4fd9-f8d1-73be16fc6ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transfer-learning-conv-ai\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transfer-learning-conv-ai'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Total 110 (delta 0), reused 0 (delta 0), pack-reused 110\u001b[K\n",
            "Receiving objects: 100% (110/110), 43.13 KiB | 1.88 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTiwvICMHao8",
        "colab_type": "text"
      },
      "source": [
        "# 여기서 부터 챗봇"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY4akf7uKPsC",
        "colab_type": "code",
        "outputId": "5473de75-28bd-4979-f03c-33cbaf2577fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My Drive/transfer-learning-conv-ai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/transfer-learning-conv-ai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YZL4r5tKTBP",
        "colab_type": "code",
        "outputId": "5a62a6ef-e48d-47b8-e616-b466b1b46b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.5.0+cu101)\n",
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
            "\u001b[?25hCollecting transformers==2.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 11.5MB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 1)) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (1.13.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 17.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 4)) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.2.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.29.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 3)) (1.16.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.1->-r requirements.txt (line 3)) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.1->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.1->-r requirements.txt (line 3)) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->-r requirements.txt (line 4)) (46.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.6.0.post3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->transformers==2.5.1->-r requirements.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->transformers==2.5.1->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=411678a44f9ce0cc87f7337019033adc7c9716de2bf72e9ad084da977a239c40\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pytorch-ignite, tokenizers, sacremoses, sentencepiece, transformers, tensorboardX\n",
            "Successfully installed pytorch-ignite-0.3.0 sacremoses-0.0.43 sentencepiece-0.1.91 tensorboardX-1.8 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rpG4e1QKZ68",
        "colab_type": "code",
        "outputId": "8dcc0b05-40ad-47e0-f0d8-cbf83bdbec84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "! python -m spacy download en"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2KBa4VAXN4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "751c4210-3d35-48a1-e191-fb098a530123"
      },
      "source": [
        "! pip install flask_wtf\n",
        "!pip install flask_restful\n",
        "!pip install flask_ngrok"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask_wtf\n",
            "  Downloading https://files.pythonhosted.org/packages/36/a9/8c01171066bd7a524ee005d81bb4a8aa446ab178043a1ad6cb5dc8f0bd83/Flask_WTF-0.14.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: itsdangerous in /usr/local/lib/python3.6/dist-packages (from flask_wtf) (1.1.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from flask_wtf) (1.1.2)\n",
            "Collecting WTForms\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/1b/af089b3d54955e0a3b0045ddd40773c2ef5dc7375ccffd09366ce469b755/WTForms-2.3.1-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->flask_wtf) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->flask_wtf) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask->flask_wtf) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.6/dist-packages (from WTForms->flask_wtf) (1.1.1)\n",
            "Installing collected packages: WTForms, flask-wtf\n",
            "Successfully installed WTForms-2.3.1 flask-wtf-0.14.3\n",
            "Collecting flask_restful\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/83/d0d33c971de2d38e54b0037136c8b8d20b9c83d308bc6c220a25162755fd/Flask_RESTful-0.3.8-py2.py3-none-any.whl\n",
            "Collecting aniso8601>=0.82\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e4/787e104b58eadc1a710738d4e418d7e599e4e778e52cb8e5d5ef6ddd5833/aniso8601-8.0.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from flask_restful) (2018.9)\n",
            "Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from flask_restful) (1.12.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask_restful) (1.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_restful) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_restful) (1.1.1)\n",
            "Installing collected packages: aniso8601, flask-restful\n",
            "Successfully installed aniso8601-8.0.0 flask-restful-0.3.8\n",
            "Collecting flask_ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2020.4.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aICbSe7Nis19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask, render_template, session, redirect, url_for, session ,request\n",
        "from flask_wtf import FlaskForm\n",
        "from wtforms import TextField,SubmitField\n",
        "from wtforms.validators import NumberRange\n",
        "import numpy as np \n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertForSequenceClassification,BertTokenizer\n",
        "import torch\n",
        "from flask_restful import Resource, Api\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ue9Q3klBxC",
        "colab_type": "code",
        "outputId": "e23f6689-b9d1-4f28-833c-6e9bc95fa84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "##여긴 챗봇 클래스\n",
        "import logging\n",
        "import random\n",
        "from argparse import ArgumentParser\n",
        "from itertools import chain\n",
        "from pprint import pformat\n",
        "import warnings\n",
        "#######\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import easydict\n",
        "\n",
        "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from train import SPECIAL_TOKENS, build_input_from_segments, add_special_tokens_\n",
        "from utils import get_dataset, download_pretrained_model\n",
        "from time import sleep\n",
        "\n",
        "class ChatBot:\n",
        "  \n",
        "  args = easydict.EasyDict({\n",
        "        \"model\": 'openai-gpt',\n",
        "        \"dataset_path\": \"data/en_book_conversational.json\",\n",
        "        \"dataset_cache\": \"./dataset_cache\",\n",
        "        \"model_checkpoint\":\"/content/drive/My Drive/transfer-learning-conv-ai/runs/May26_10-33-33_ime-502_openai-gpt\",\n",
        "        \"temperature\": 1,\n",
        "        \"top_k\": 70,\n",
        "        \"top_p\": 0.5,\n",
        "        \"max_history\": 2,\n",
        "        \"device\" : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"no_sample\": True,\n",
        "        \"max_length\": 20, \n",
        "        \"min_length\" :1, \n",
        "        \"seed\": 0\n",
        "})  \n",
        "  if args.model_checkpoint == \"\":\n",
        "    if args.model == 'gpt2':\n",
        "      raise ValueError(\"Interacting with GPT2 requires passing a finetuned model_checkpoint\")\n",
        "  else:\n",
        "    args.model_checkpoint = download_pretrained_model()\n",
        "    if args.seed != 0:\n",
        "      random.seed(args.seed)\n",
        "      torch.random.manual_seed(args.seed)\n",
        "      torch.cuda.manual_seed(args.seed)\n",
        "    # logger.info(\"Get pretrained model and tokenizer\")\n",
        "    tokenizer_class, model_class = (GPT2Tokenizer, GPT2LMHeadModel) if args.model == 'gpt2' else (OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
        "    model = model_class.from_pretrained(args.model_checkpoint)\n",
        "    model.to(args.device)\n",
        "    add_special_tokens_(model, tokenizer)\n",
        "    # logger.info(\"Sample a personality\")\n",
        "    dataset = get_dataset(tokenizer, args.dataset_path,args.dataset_cache)\n",
        "    personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "    personality = random.choice(personalities)\n",
        "    history=[]\n",
        "    #\n",
        "    contents=[]\n",
        "    ids_to_tokens=[]\n",
        "    for i in personality:\n",
        "      ids_to_tokens.append(tokenizer.convert_ids_to_tokens(i))\n",
        "    # ids_to_tokens=[tokenizer.convert_ids_to_tokens(i) for i in  personality]\n",
        "    contents_tokens=[\" \".join(sent) for sent in ids_to_tokens]\n",
        "    contents = [sentence.replace(\"</w>\",\"\")for sentence in contents_tokens]\n",
        "    print(contents)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
        "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
        "                whose total probability mass is greater than or equal to the threshold top_p.\n",
        "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
        "                the threshold top_p.\n",
        "            threshold: a minimal threshold to keep logits\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Back to unsorted indices and set them to -infinity\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    indices_to_remove = logits < threshold\n",
        "    logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits\n",
        "\n",
        "  @staticmethod\n",
        "  def sample_sequence(personality, history, tokenizer, model, args, current_output=None):\n",
        "      special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "      if current_output is None:\n",
        "          current_output = []\n",
        "\n",
        "      for i in range(args.max_length):\n",
        "          instance = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
        "\n",
        "          input_ids = torch.tensor(instance[\"input_ids\"], device=args.device).unsqueeze(0)\n",
        "          token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args.device).unsqueeze(0)\n",
        "\n",
        "          logits = model(input_ids, token_type_ids=token_type_ids)\n",
        "          if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
        "              logits = logits[0]\n",
        "          logits = logits[0, -1, :] / args.temperature\n",
        "          logits = ChatBot.top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
        "          sleep(5)\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "          prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n",
        "          if i < args.min_length and prev.item() in special_tokens_ids:\n",
        "              while prev.item() in special_tokens_ids:\n",
        "                  if probs.max().item() == 1:\n",
        "                      warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
        "                      break  # avoid infinitely looping over special token\n",
        "                  prev = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "          if prev.item() in special_tokens_ids:\n",
        "              break\n",
        "          current_output.append(prev.item())\n",
        "\n",
        "      return current_output\n",
        "  \n",
        "  @classmethod\n",
        "  def return_message(cls,sample_json):\n",
        "    sentence=sample_json['text']\n",
        "    while True:\n",
        "        raw_text = sentence\n",
        "        while not raw_text:\n",
        "            print('Prompt should not be empty!')\n",
        "            raw_text = sentence\n",
        "        \n",
        "        cls.history.append(cls.tokenizer.encode(raw_text))\n",
        "        with torch.no_grad():\n",
        "            out_ids = cls.sample_sequence(cls.personality, cls.history, cls.tokenizer, cls.model, cls.args)\n",
        "        cls.history.append(out_ids)\n",
        "        cls.history = cls.history[-(2*cls.args.max_history+1):]\n",
        "        out_text = cls.tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "        return out_text"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['all right say cheese', 'just show me what to do with the camera .', 'my phones battery is low .', 'i take you photo .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4tSHEZ3iS_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from . import ChatBot\n",
        "\n",
        "class AFL:\n",
        "  count=0\n",
        "  changed_flag=False\n",
        "  contents =ChatBot.contents\n",
        "\n",
        "  def __init__(self,model,tokenizer):\n",
        "    self.model = model\n",
        "    self.tokenizer= tokenizer\n",
        "    self.score=[]\n",
        "   \n",
        "    \n",
        "\n",
        "  def return_prediction(self,sample_json):\n",
        "    sen_1 = sample_json['text']\n",
        "    out_put= []\n",
        "    if self.model == M_model:\n",
        "      print(self.model)\n",
        "      # personalities=[\"may i help you yes please.\", \"do you have long - sleeve shirts?\", \"yes\", \"they are right here. how much are they? \",\"they are 15 dollars each do you have any larger sizes what about these?\", \"there are larger and more colorful.\", \"this one looks good. can i try this on? sure.\" ,\"the fitting room is over there.\", \"what do you think? that looks really good on you\", \"thanks.\", \"i will take this\"] \n",
        "\n",
        "      classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "      for i in AFL.contents:\n",
        "        sequence_persona=i\n",
        "        paraphrase = self.tokenizer.encode_plus(sen_1, sequence_persona, return_tensors=\"pt\")\n",
        "        paraphrase_classification_logits = self.model(**paraphrase)[0]\n",
        "        paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "        out_put.append(paraphrase_results[1])\n",
        "      out_put=np.array(out_put)\n",
        "      out_put_max=np.max(out_put)\n",
        "      self.score.append(round(out_put_max*100))\n",
        "      # if AFL.count==10:\n",
        "      #   avg =average(self.score,AFL.count)\n",
        "      #   return avg\n",
        "      \n",
        "      results = {\"max_value\":round(out_put_max*100)}\n",
        "\n",
        "      return round(out_put_max*100)\n",
        "\n",
        "    elif self.model == C_model:\n",
        "      classes = [\"wrong\", \"correct\"]\n",
        "      paraphrase = self.tokenizer.encode_plus(sen_1, return_tensors=\"pt\")\n",
        "      paraphrase_classification_logits = self.model(**paraphrase)[0]\n",
        "      paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "      self.score.append(round(paraphrase_results[1] * 100))\n",
        "      # if AFL.count==10:\n",
        "      #   avg =average(self.score,AFL.count)\n",
        "      #   return avg\n",
        "      # results = {classes[1]: paraphrase_results[1]}\n",
        "      print(self.score)\n",
        "      return round(paraphrase_results[1] * 100)\n",
        "  \n",
        "  def average(self):\n",
        "    return sum(self.score)/AFL.count\n",
        "  \n",
        "  @classmethod\n",
        "  def change_content(cls,CoLA,MRPC):\n",
        "    CoLA_avg = CoLA.average()\n",
        "    MRPC_avg = MRPC.average()\n",
        "    print(CoLA_avg,MRPC_avg)\n",
        "    if CoLA_avg >80 and MRPC_avg > 70 and AFL.changed_flag ==False :\n",
        "      ChatBot.personality = random.choice(ChatBot.personalities)\n",
        "      contents=[]\n",
        "      ids_to_tokens=[]\n",
        "      ids_to_tokens=[ChatBot.tokenizer.convert_ids_to_tokens(i) for i in  ChatBot.personality]\n",
        "      contents_tokens=[\" \".join(sent) for sent in ids_to_tokens]\n",
        "      contents = [sentence.replace(\"</w>\",\"\")for sentence in contents_tokens]\n",
        "      # print(ChatBot.personality)\n",
        "      AFL.changed_flag=True\n",
        "      print(ChatBot.contents)\n",
        "      print('_____________')\n",
        "      print(contents)\n",
        "    return contents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_4pimBeiS2p",
        "colab_type": "code",
        "outputId": "92f75de6-f978-4ca3-841b-44f5164d1d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from . import AFL\n",
        "from .import ChatBot\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  \n",
        "\n",
        "\n",
        "\n",
        "M_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "M_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "C_model =  BertForSequenceClassification.from_pretrained(\"/content/drive/My Drive/CoLAoutput\")\n",
        "C_tokenizer= BertTokenizer.from_pretrained(\"/content/drive/My Drive/CoLAoutput\")  \n",
        "\n",
        "Init_Chatbot=ChatBot()\n",
        "\n",
        "\n",
        "\n",
        "CoLA =AFL(C_model,C_tokenizer)\n",
        "MRPC =AFL(M_model,M_tokenizer)\n",
        "\n",
        "\n",
        "@app.route('/prediction', methods = ['POST'])\n",
        "def prediction():\n",
        "    if AFL.count >=1:\n",
        "      AFL.contents = AFL.change_content(CoLA,MRPC)\n",
        "      # AFL.count =0\n",
        "    sentence = request.get_json()#json 데이터를 받아옴\n",
        "    result_conv=ChatBot.return_message(sample_json=sentence)\n",
        "    result_mrpc =MRPC.return_prediction(sample_json=sentence)\n",
        "    result_cola =CoLA.return_prediction(sample_json=sentence)\n",
        "    AFL.count+=1\n",
        "    results={\"sentence\": result_conv,\"similarity\":result_mrpc,\"correct\":result_cola ,\"contents\":AFL.contents ,\"count!!\":AFL.count }\n",
        "    AFL.changed_flag=False\n",
        "    return jsonify(results)# 받아온 데이터를 다시 전송\n",
        " \n",
        "\n",
        "# @app.route('/prediction/change')\n",
        "# def change_content():\n",
        "\n",
        "\n",
        "\n",
        "app.run()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://d5cc90fdaa35.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [31/May/2020 14:43:47] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[97]\n",
            "97.0 18.0\n",
            "['all right say cheese', 'just show me what to do with the camera .', 'my phones battery is low .', 'i take you photo .']\n",
            "_____________\n",
            "['it is 20 minutes from here .', 'its 19 dollars .', 'we will be there soon .', 'id like to go to the hotel .']\n",
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [31/May/2020 14:44:27] \"\u001b[37mPOST /prediction HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[97, 97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcWffeViy_il",
        "colab_type": "code",
        "outputId": "f7244d6d-4ced-41b3-cc28-afc9f4118ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}